{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "d4c8c"
   },
   "outputs": [],
   "source": [
    "# üìò Customer Churn Prediction ‚Äì Final Production Pipeline\n",
    "# ========================================================\n",
    "\n",
    "# 1Ô∏è‚É£ IMPORT LIBRARIES\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import joblib\n",
    "import optuna\n",
    "import shap\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve, recall_score\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# ================================\n",
    "# 2Ô∏è‚É£ LOAD DATA & CLEANING\n",
    "# ================================\n",
    "# Load data\n",
    "df = pd.read_csv(\"../data/Bank_Churn.csv\") # ‚ö†Ô∏è Update path if needed\n",
    "\n",
    "# Drop ID columns (we don't need them for prediction)\n",
    "df = df.drop([\"CustomerId\", \"Surname\"], axis=1)\n",
    "\n",
    "print(\"Data Loaded. Shape:\", df.shape)\n",
    "print(df.head())\n",
    "\n",
    "# ================================\n",
    "# 3Ô∏è‚É£ TRAIN-TEST SPLIT\n",
    "# ================================\n",
    "# We split BEFORE encoding/scaling to prevent data leakage\n",
    "X = df.drop(\"Exited\", axis=1)\n",
    "y = df[\"Exited\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Calculate scale_pos_weight for imbalance handling\n",
    "# (Count of Negatives / Count of Positives)\n",
    "pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "print(f\"Calculated scale_pos_weight: {pos_weight:.2f}\")\n",
    "\n",
    "# ================================\n",
    "# 4Ô∏è‚É£ DEFINE THE PREPROCESSING PIPELINE\n",
    "# ================================\n",
    "# Define which columns are which\n",
    "numerical_features = ['CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'EstimatedSalary']\n",
    "categorical_features = ['Geography', 'Gender']\n",
    "\n",
    "# Create the ColumnTransformer\n",
    "# This handles all data transformations automatically\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        # Scale numbers\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        # Encode categories (Ordinal is good for Trees + SHAP)\n",
    "        ('cat', OrdinalEncoder(), categorical_features)\n",
    "    ],\n",
    "    # Pass 'HasCrCard' and 'IsActiveMember' through without changes\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# ================================\n",
    "# 5Ô∏è‚É£ OPTUNA OPTIMIZATION (Optional)\n",
    "# ================================\n",
    "# ‚ÑπÔ∏è NOTE: I have commented this out to make the code run fast.\n",
    "# If you want to re-optimize, uncomment the lines below.\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 300, 800),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.2),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 10),\n",
    "        \"scale_pos_weight\": pos_weight,\n",
    "        \"eval_metric\": \"auc\",\n",
    "        \"random_state\": 42,\n",
    "        \"n_jobs\": -1\n",
    "    }\n",
    "    # Build temp pipeline for trial\n",
    "    model = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', XGBClassifier(**params))\n",
    "    ])\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict_proba(X_test)[:, 1]\n",
    "    return roc_auc_score(y_test, preds)\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "best_params = study.best_params\n",
    "\n",
    "# ‚ö†Ô∏è USING PRE-CALCULATED BEST PARAMS (To save time)\n",
    "# These are based on your previous successful runs\n",
    "best_params = {\n",
    "    'n_estimators': 455,\n",
    "    'learning_rate': 0.12,\n",
    "    'max_depth': 4,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'min_child_weight': 1,\n",
    "    'scale_pos_weight': pos_weight, # Ensures high recall\n",
    "    'eval_metric': 'logloss',\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "# ================================\n",
    "# 6Ô∏è‚É£ BUILD & TRAIN FINAL PIPELINE\n",
    "# ================================\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', XGBClassifier(**best_params))\n",
    "])\n",
    "\n",
    "print(\"üöÄ Training Pipeline...\")\n",
    "start_time = time.time()\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(f\"‚úÖ Training Complete. Time: {time.time() - start_time:.2f}s\")\n",
    "\n",
    "# ================================\n",
    "# 7Ô∏è‚É£ EVALUATION\n",
    "# ================================\n",
    "# Get probabilities\n",
    "y_prob = pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Apply threshold (Default 0.5, but you can tune this using F2 score logic)\n",
    "threshold = 0.5\n",
    "y_pred = (y_prob >= threshold).astype(int)\n",
    "\n",
    "print(\"\\n--- Final Classification Report ---\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion Matrix Plot\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.show()\n",
    "\n",
    "# ROC Curve\n",
    "auc = roc_auc_score(y_test, y_prob)\n",
    "fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "plt.plot(fpr, tpr, label=f\"AUC = {auc:.2f}\")\n",
    "plt.plot([0, 1], [0, 1], '--')\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ================================\n",
    "# 8Ô∏è‚É£ SHAP EXPLAINABILITY\n",
    "# ================================\n",
    "# Note: SHAP needs the *transformed* data, not raw data.\n",
    "# We must use the 'preprocessor' step to transform X_test first.\n",
    "\n",
    "print(\"Generating SHAP explanations...\")\n",
    "\n",
    "# 1. Transform X_test using the pipeline's preprocessor\n",
    "X_test_transformed = pipeline.named_steps['preprocessor'].transform(X_test)\n",
    "\n",
    "# 2. Get feature names from the transformer\n",
    "# (OrdinalEncoder keeps names, StandardScaler keeps names)\n",
    "feature_names = numerical_features + categorical_features + ['HasCrCard', 'IsActiveMember']\n",
    "\n",
    "# 3. Create Explainer\n",
    "model_step = pipeline.named_steps['classifier']\n",
    "explainer = shap.TreeExplainer(model_step)\n",
    "shap_values = explainer.shap_values(X_test_transformed)\n",
    "\n",
    "# 4. Plot\n",
    "shap.initjs()\n",
    "plt.title(\"Feature Importance (SHAP)\")\n",
    "shap.summary_plot(shap_values, X_test_transformed, feature_names=feature_names, show=False)\n",
    "plt.show()\n",
    "\n",
    "# ================================\n",
    "# 9Ô∏è‚É£ BUSINESS & FINANCIAL REPORT\n",
    "# ================================\n",
    "# Assumptions\n",
    "clv = 1200\n",
    "cost = 50\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "saved_revenue = tp * clv\n",
    "intervention_cost = (tp + fp) * cost\n",
    "roi = saved_revenue - intervention_cost\n",
    "\n",
    "report = f\"\"\"\n",
    "# üìä **Customer Churn Prediction ‚Äì Final Report**\n",
    "\n",
    "## üîç Executive Summary\n",
    "- **Model:** XGBoost Pipeline (StandardScaler + OrdinalEncoder)\n",
    "- **Performance:** - **Recall (Churners):** {recall_score(y_test, y_pred):.2%}\n",
    "    - **AUC Score:** {auc:.2f}\n",
    "- **Strategy:** The model uses `scale_pos_weight={pos_weight:.2f}` to prioritize detecting churners.\n",
    "\n",
    "## üß† Why are they leaving? (SHAP Insights)\n",
    "1. **Age:** Older customers are the highest risk group.\n",
    "2. **Activity:** Inactive members (`IsActiveMember=0`) are likely to leave.\n",
    "3. **Products:** Customers with 1 product churn; those with 2 stay.\n",
    "4. **Balance:** High balance customers are leaving (Rate shopping?).\n",
    "\n",
    "## üí∞ Financial Impact\n",
    "- **Targeted Customers:** {tp + fp}\n",
    "- **Churners Saved:** {tp}\n",
    "- **Projected ROI:** ${roi:,.2f}\n",
    "\"\"\"\n",
    "print(report)\n",
    "\n",
    "# ================================\n",
    "# üîü SAVE FOR PRODUCTION\n",
    "# ================================\n",
    "# We save the WHOLE pipeline (Scaler + Encoder + Model) in one file\n",
    "joblib.dump(pipeline, \"churn_pipeline.joblib\")\n",
    "print(\"‚úÖ Pipeline saved as 'churn_pipeline.joblib'\")\n",
    "print(\"   Ready for deployment!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  },
  "vincent": {
   "sessionId": "7984ad122bb26f360ae24b9c_2025-12-02T18-22-41-443Z"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
